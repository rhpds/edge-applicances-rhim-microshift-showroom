= Module 4: Safe Updates & Rollbacks
:page-nav-title: Performing an Atomic Update
:icons: font
:source-highlighter: rouge
:experimental:

== Learning objectives
By the end of this module, you will be able to:

* Explain the A/B boot model and atomic update process
* Build an updated appliance image (4.20)
* Perform an update using disconnected workflows
* Export and import images using directory transport
* Verify successful updates using greenboot
* Perform manual rollbacks and validate recovery
* Understand automatic rollback and failure protection

[#know_section]
[#atomic_updates]
== 4.1 Know: Solving Risky Updates with Atomic Operations

=== The Business Challenge

ManufacturingCo struggles with updates:

* 15% updates fail and require onsite IT
* 2–4 hours of downtime per failure
* Debugging in the field is difficult
* Network issues make updates unpredictable

**The Real Issue:**  
Traditional package-based updates mutate a live system. If something breaks, you’re stuck diagnosing on a production appliance — often in a remote factory.

=== Atomic Updates: The New Model

RHEL Image Mode replaces package updates with **atomic image deployment**.

Each system maintains two bootable states:

* **A — Active (current boot)**
* **B — Next boot candidate**

When you update:

1. A new OS image is pulled/staged
2. Nothing changes until reboot
3. At reboot the new image becomes active
4. If anything fails → auto rollback to A

**Business Value**

* Safe updates — no partial installs
* Instant rollback — recovery in **seconds**
* No onsite IT — greenboot handles failure
* Confidence to update more frequently

=== Why This Matters

**Traditional approach**

* Risky in-place mutation
* 15% failures + site visits
* Manual recovery steps

**Image Mode approach**

* Immutable, versioned upgrade

* <1% failure rate
* Automatic rollback
* Zero downtime risk

For ManufacturingCo, this eliminates onsite IT trips and turns updates from drama into routine.

[#show_section]
[#build_4.20]
== 4.2 Show: Build the Updated Image (4.20)

=== Why We're Doing This

To demonstrate **real update workflow**:

* Build a new appliance release (4.20)
* Stage it as the next boot image
* Prepare to roll forward — or back — instantly

In the real world, new versions might include:

* Security fixes
* Updated MicroShift payload
* New container workloads
* Configuration changes

=== View the Updated Containerfile

. Inspect the `Containerfile.4.20`:
+
[source,sh,role=execute]
----
cat Containerfile.4.20
----
+
[%collapsible]
====
.Output of `$cat Containerfile.4.20`
[source,dockerfile]
----
FROM localhost/microshift-bootc-embeeded:4.19
COPY ./embed_image.sh /usr/bin/
## Needed as bootc-image-builder requires a repo file to be present in the base image.
# in the build.sh with TAG=4.20, we replace the 4.19 repo with the 4.20 repo to allow upgrade MicroShift to 4.20. 
ADD redhat.repo /etc/yum.repos.d/redhat.repo
RUN dnf update --enablerepo=rhel-9-for-$(uname -m)-baseos-eus-rpms --enablerepo=rhel-9-for-x86_64-appstream-eus-rpms -y --releasever=9.6 && \
    dnf clean all
RUN > /usr/lib/containers/storage/image-list.txt
# Pull the container images into /usr/lib/containers/storage:
# - Each image goes into a separate sub-directory
# - Sub-directories are named after the image reference string SHA
# - An image list file maps image references to their name SHA
# First for MicroShift payload
RUN --mount=type=secret,id=pullsecret,dst=/run/secrets/pull-secret.json \
    images="$(jq -r ".images[]" /usr/share/microshift/release/release-"$(uname -m)".json)" ; \
    mkdir -p "${IMAGE_STORAGE_DIR}" ; \
    for img in ${images} ; do \
       /usr/bin/embed_image.sh ${img} --authfile /run/secrets/pull-secret.json ; \
     done 
RUN cat /usr/share/microshift/release/release-"$(uname -m)".json|jq .
# Then for Applications
RUN --mount=type=secret,id=pullsecret,dst=/run/secrets/pull-secret.json <<PULL
    /usr/bin/embed_image.sh docker.io/library/wordpress:6.2.1-apache
    /usr/bin/embed_image.sh docker.io/library/mysql:8.0
PULL

----
====

=== Build the 4.20 Image

. Run the 4.20 build:
+
[source,sh,role=execute]
----
time sudo bash -x build.sh 4.20 2>&1 | tee build-4.20.log
----

[NOTE]
====
Every new image is just another tag — making OS updates feel like container deployments.
====

[NOTE]
====
**Image Size Reality Check**:

Layering 4.20 on 4.19 increases size.  
In production, you may build **fresh images** instead.
Both models work with `bootc switch`.
====

=== How This Solves the Problem

We now have:

* **Two appliance versions**

* Known-good 4.19 running live
* New 4.20 ready to deploy safely

This is the atomic foundation — upgrade confidently, rollback instantly.

[#upgrade]
== 4.3 Show: Deploy Update Using Disconnected Workflow

=== Why We're Doing This

This demonstrates **update workflows for the real edge**:

* No registry
* No internet
* No on-site IT
* Same appliance everywhere

We'll:

1. Export the image as a directory
2. Transfer it offline
3. Use bootc to switch safely
4. Reboot into the new version

=== Export the 4.20 Image

. Make an export directory:
+
[source,sh,role=execute]
----
mkdir -p /var/tmp/microshift-bootc-4.20
----

. Export using `skopeo`:
+
[source,sh,role=execute]
----
sudo skopeo copy containers-storage:localhost/microshift-bootc-embeeded:4.20 dir:/var/tmp/microshift-bootc-4.20
----

. Pack (tar) the directory:
+
[source,sh,role=execute]
----
cd /var/tmp/; time tar cvf microshift-bootc-4.20.tar microshift-bootc-4.20/
----

. Check size (~11GB):
+
[source,sh,role=execute]
----
du -sh /var/tmp/microshift-bootc-4.20.tar
----

=== Transfer to the Edge Node

. Copy directory to appliance:
+
[source,sh,role=execute]
----
scp /var/tmp/microshift-bootc-4.20.tar redhat@<VM_IP_ADDRESS>:/var/tmp
----

[NOTE]
====
In real deployments: USB sticks, courier delivery, portable SSDs — all valid.
====

=== Switch to the New Image

. SSH to the VM:
+
[source,sh,role=execute]
----
ssh redhat@<VM_IP_ADDRESS>
----

. Check the current image:
+
[source,sh,role=execute]
----
sudo bootc status
----

. Unpack (un-tar) the directory:
+
[source,sh,role=execute]
----
cd /var/tmp/; time tar -xvf microshift-bootc-4.20.tar
----

. Switch to the locally transferred 4.20:
+
[source,sh,role=execute]
----
sudo bootc switch --transport dir /var/tmp/microshift-bootc-4.20/
----

. Confirm it staged:
+
[source,sh,role=execute]
----
sudo bootc status
sudo rpm-ostree status
----

=== Activate the Update

. Apply and reboot:
+
[source,sh,role=execute]
----
sudo bootc upgrade --apply
----

Reconnect after boot completes.

=== Verify Success

. Check the active deployment:
+
[source,sh,role=execute]
----
sudo bootc status
----
. Check MicroShift:
+
[source,sh,role=execute]
----
microshift version
oc get pods -A
----

. Check greenboot:
+
[source,sh,role=execute]
----
systemctl status greenboot-healthcheck.service
journalctl -u greenboot-healthcheck.service
----

=== Why This Solves the Problem

You just proved:

* Updates work **without a registry**
* Failure detection is automatic
* Staging and activation are safe
* Recovery requires no IT staff

[#rollback]
== 4.4 Show: Demonstrate Rollback Capability

=== Why Rollback Matters

This is the real magic of atomic updates.

Rollback gives:

* Instant safety net
* Zero data loss
* Zero downtime risk
* No field remediation

=== Check Rollback Availability

. See active + rollback deployments:
+
[source,sh,role=execute]
----
sudo bootc status
----

You should see:

* **Booted:** 4.20
* **Rollback target:** 4.19

=== Perform the Rollback

. Stage rollback:
+
[source,sh,role=execute]
----
sudo bootc rollback
----

. Reboot:
+
[source,sh,role=execute]
----
sudo systemctl reboot
----

=== Verify Rollback

. Verify booted image:
+
[source,sh,role=execute]
----
sudo bootc status
microshift version
oc get pods -A
----

Expected:

* Booted image = 4.19
* MicroShift back to 4.19
* Workloads still running

[IMPORTANT]
====
Rollback preserves:

- `/etc`  
- `/var`  
- Data + config  
- Application workloads  
====

=== The Practical Outcome

ManufacturingCo can:

* Deploy updates at scale
* Trust automatic rollback
* Avoid onsite technicians
* Recover from failure in under a minute

[#validation]
== 4.5 Validation: Update and Rollback Verified

=== Validate Success

Your results should show:

* 4.20 deployed successfully
* Greenboot reports healthy
* Pods continued running
* Rollback target present
* Successful rollback to 4.19

=== Validation Checklist

* Updated to 4.20  
* MicroShift upgraded  
* Greenboot passed  
* Apps survived upgrade  
* Rollback target intact  
* Rolled back to 4.19  
* No onsite intervention needed

[#delta_updates]
== 4.6 Know: Experimental: Delta Updates for Low-Bandwidth

=== Why This Exists

Disconnected edge sites often face:

* Limited bandwidth
* Expensive connections
* No registry access

Full image transfers = multiple GB  
Delta updates = <500MB  
Same update — drastically smaller file.

=== When to Use

Use **full images** when:

* USB or local transfer is easy
* Bandwidth is not the bottleneck

Use **delta updates** when:

* Remote sites use cellular/satellite
* Network transfers are expensive or slow
* Bandwidth limits prevent GB moves
* Sites need regular patch cadence

[NOTE]
====
Delta updates are **experimental today**, but highly promising for scaling fleets.
====

=== How It Works

* Create delta in a development environment
* Transfer delta file
* Rebuild the new image on the appliance
* Apply with `bootc switch`

=== Future Direction

Red Hat is actively working on native delta update support (see link:https://issues.redhat.com/browse/RHELBU-3333[RHELBU-3333^]). The goal is to provide:

* Unified tooling that works with standard container formats
* Integration with bootc workflows
* Support for air-gapped and low-bandwidth scenarios
* Alignment with industry standards (Flatpak OCI delta spec)

When available, delta updates will become a first-class feature for edge appliance management.

---

**Next:** Proceed to xref:module-05.adoc[Module 5: Business Outcomes & Resources] and connect what you built to customer value.
