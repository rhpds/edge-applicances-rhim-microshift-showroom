= Module 3: Deploying to Edge Locations
:page-nav-title: Deploy & Verify Appliance
:icons: font
:source-highlighter: rouge
:experimental:

== Learning objectives
By the end of this module, you will be able to:

* Create an isolated network for disconnected environment simulation
* Deploy a bootc image to a virtual machine using KVM
* Configure kubeconfig for MicroShift access
* Verify a self-contained appliance is running correctly
* Confirm embedded container images are being used (not pulled from network)

[#know_section]
[#deploying]
== 3.1 Know: Deploying to Edge Locations

=== The Business Challenge

Remember ManufacturingCo? They need to deploy appliances to 50 factory locations across North America. Each location has different constraints:

* Some locations have no internet connectivity
* Others have intermittent or low-bandwidth connections
* Physical access may be limited
* IT staff may not be available on-site

**The Challenge:** How do we get self-contained appliances to edge locations reliably and consistently, regardless of network connectivity?

=== How ISO-Based Deployment Solves This

ISO-based deployment enables:

* **USB/Physical Media Deployment:** Copy ISO to USB drive, deploy to any location
* **Network Deployment:** Use PXE boot or network file share for connected locations
* **Consistent Installation:** Same ISO works identically across all locations
* **Offline Operation:** Once installed, appliance operates completely offline

This solves the deployment challenge by providing a single, portable file that contains everything needed—no network dependency during or after installation.

=== Why Simulate a Disconnected Environment

In this lab, we'll create an isolated network to simulate a disconnected edge location. This proves that:

* The appliance boots and runs without internet access
* Embedded container images are used (not pulled from registries)
* Applications start successfully using only embedded images
* The appliance operates exactly as it would in a real disconnected factory location

This simulation validates that ManufacturingCo can deploy to any location, regardless of connectivity.

[#show_section]
[#boot_vm]
== 3.2 Show: Deploy the Appliance to Target System

=== Why We're Doing This

This simulates deploying to an edge location. By creating an isolated network and deploying the ISO to a VM, we're demonstrating how ManufacturingCo would deploy appliances to their factory locations—whether via USB, network share, or PXE boot.

=== Create Isolated Network

First, we'll create an isolated network to simulate a disconnected environment:

. View the Create Network script:
+
[source,sh,role=execute]
----
cat setup-isolated-network.sh
----
+
[%collapsible]
====
.Output of `setup-isolated-network.sh`
[source,bash]
----
#!/bin/bash
#
# Script to create an isolated KVM network (no internet access)
# and optionally attach a VM to it
#

set -e

# Configuration
NETWORK_NAME="bootc-isolated"
BRIDGE_NAME="virbr-bootc"
NETWORK_IP="192.168.100.1"
NETWORK_NETMASK="255.255.255.0"
NETWORK_RANGE_START="192.168.100.2"
NETWORK_RANGE_END="192.168.100.254"
NETWORK_XML="/tmp/${NETWORK_NAME}.xml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if running as root or with sudo
if [ "$EUID" -ne 0 ]; then 
    print_error "Please run as root or with sudo"
    exit 1
fi

# Check if network already exists (defined or active)
if virsh net-info "${NETWORK_NAME}" &>/dev/null || virsh net-list --all --name | grep -q "^${NETWORK_NAME}$"; then
    print_warn "Network '${NETWORK_NAME}' already exists"
    read -p "Do you want to destroy and recreate it? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        print_info "Destroying existing network..."
        # Try to destroy if active
        if virsh net-destroy "${NETWORK_NAME}" &>/dev/null; then
            print_info "Network destroyed"
        fi
        # Undefine the network
        if virsh net-undefine "${NETWORK_NAME}" &>/dev/null; then
            print_info "Network undefined"
        fi
    else
        print_info "Exiting without changes"
        exit 0
    fi
fi

# Create network XML file
print_info "Creating isolated network XML definition..."
cat > "${NETWORK_XML}" <<EOF
<network>
  <name>${NETWORK_NAME}</name>
  <uuid>$(uuidgen)</uuid>
  <forward mode='none'/>
  <bridge name='${BRIDGE_NAME}' stp='on' delay='0'/>
  <ip address='${NETWORK_IP}' netmask='${NETWORK_NETMASK}'>
    <dhcp>
      <range start='${NETWORK_RANGE_START}' end='${NETWORK_RANGE_END}'/>
    </dhcp>
  </ip>
</network>
EOF

print_info "Network XML created at ${NETWORK_XML}"

# Define the network
print_info "Defining network '${NETWORK_NAME}'..."
virsh net-define "${NETWORK_XML}"

# Start the network
print_info "Starting network '${NETWORK_NAME}'..."
virsh net-start "${NETWORK_NAME}"

# Set network to autostart
print_info "Setting network '${NETWORK_NAME}' to autostart..."
virsh net-autostart "${NETWORK_NAME}"

# Verify network is active
if virsh net-info "${NETWORK_NAME}" | grep -q "Active:.*yes"; then
    print_info "Network '${NETWORK_NAME}' is now active and isolated (no internet access)"
    print_info "Network details:"
    virsh net-info "${NETWORK_NAME}"
    echo
    print_info "Network IP range: ${NETWORK_RANGE_START} - ${NETWORK_RANGE_END}"
    print_info "Gateway/DNS: ${NETWORK_IP}"
else
    print_error "Failed to start network"
    exit 1
fi

# Clean up temporary XML file
rm -f "${NETWORK_XML}"

print_info "Isolated network setup complete!"
print_info "To use this network in create-vm.sh, set NETNAME='${NETWORK_NAME}'"
----
====

. Create a new isolated network:
+
[source,sh,role=execute]
----
sudo bash setup-isolated-network.sh
----

=== Create and Boot the VM

. View the VM creation script:
+
[source,sh,role=execute]
----
cat create-vm.sh
----
+
[%collapsible]
====
.Output of `create-vm.sh`
[source,bash]
----
VMNAME=microshift-4.19-bootc-vm1
NETNAME=bootc-isolated
ISO_FILE="microshift-bootc-embeeded-4.19.iso"
LIBVIRT_IMAGES_DIR="/var/lib/libvirt/images"
KS_FILE="kickstart.ks"

# Get the directory where the script is located or use current directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SOURCE_ISO="${SCRIPT_DIR}/${ISO_FILE}"
TARGET_ISO="${LIBVIRT_IMAGES_DIR}/${ISO_FILE}"
SOURCE_KS="${SCRIPT_DIR}/${KS_FILE}"

# Check if source ISO file exists
if [ ! -f "${SOURCE_ISO}" ]; then
    echo "Error: ISO file not found at ${SOURCE_ISO}"
    echo "Please ensure the ISO file exists or update ISO_FILE variable"
    exit 1
fi

# Check if running as root or with sudo
if [ "$EUID" -ne 0 ]; then 
    echo "Error: Please run as root or with sudo"
    exit 1
fi

# Ensure libvirt images directory exists
mkdir -p "${LIBVIRT_IMAGES_DIR}"

# Copy ISO to libvirt images directory if it doesn't exist or is different
if [ ! -f "${TARGET_ISO}" ] || [ "${SOURCE_ISO}" -nt "${TARGET_ISO}" ]; then
    echo "Copying ISO to ${TARGET_ISO} (this may take a while for large files)..."
    cp "${SOURCE_ISO}" "${TARGET_ISO}"
    chmod 644 "${TARGET_ISO}"
    echo "ISO copied successfully"
else
    echo "ISO already exists at ${TARGET_ISO}, skipping copy"
fi

# Create kickstart file with LVM partitioning
echo "Creating kickstart file with LVM partitioning..."
cat > "${SOURCE_KS}" <<'EOFKS'
lang en_US.UTF-8
keyboard us
timezone UTC
text
reboot

# Partition the disk with hardware-specific boot and swap partitions, adding an
# LVM volume that contains a 10GB+ system root. The remainder of the volume will
# be used by the CSI driver for storing data.
zerombr
clearpart --all --initlabel
# Create boot and swap partitions as required by the current hardware platform
reqpart --add-boot
# Add an LVM volume group and allocate a system root logical volume
part pv.01 --grow
volgroup rhel pv.01
logvol / --vgname=rhel --fstype=xfs --size=50240 --name=root

# Lock root user account
rootpw --lock

# Configure network to use DHCP and activate on boot
network --bootproto=dhcp --device=link --activate --onboot=on

# Configure bootc to install from the local embedded container repository.
# See /osbuild-base.ks on ISO images generated by bootc-image-builder.
ostreecontainer --transport oci --url /run/install/repo/container

%post --log=/dev/console --erroronfail
%end
EOFKS
echo "Kickstart file created at ${SOURCE_KS}"

# Create the VM using location with kernel/initrd from ISO
virt-install --name ${VMNAME} \
--os-variant fedora-coreos-stable \
--memory 8192 \
--vcpus 4 \
--disk size=120 \
--network network=${NETNAME} \
--location "${TARGET_ISO},kernel=images/pxeboot/vmlinuz,initrd=images/pxeboot/initrd.img" \
--initrd-inject "${SOURCE_KS}" \
--extra-args "inst.ks=file:/kickstart.ks console=ttyS0" \
--serial pty \
--console pty,target_type=serial \
--wait

# Clean up temporary kickstart file
rm -f "${SOURCE_KS}"
----
====

[NOTE]
====
This script creates a VM, attaches your self-contained ISO, injects the Kickstart file, and starts the automated installation. Kickstart automates the OS installation so the VM can boot straight into your Image Mode appliance with zero manual clicks.
====

. Create and start the VM:
+
[source,sh,role=execute]
----
sudo bash create-vm.sh
----

Once the VM begins booting, open a new terminal and connect to the console to watch the installation process, which should take 5-10 minutes to conclude:

[source,sh,role=execute]
----
sudo virsh console microshift-4.19-bootc-vm1
----

You should now see the appliance boot into Image Mode.

=== Verify VM is Running

Before proceeding to verification, confirm the VM is running:

. Verify the VM is in running state:
+
[source,sh,role=execute]
----
sudo virsh list --all | grep microshift-4.19-bootc-vm1
----

[IMPORTANT]
====
**Expected results:**
* VM `microshift-4.19-bootc-vm1` should show state as "running"
* If the VM is not running, check the console output or logs for installation errors
====

=== How This Solves the Problem

By deploying the ISO to a VM on an isolated network, we've demonstrated that:

* The appliance can be deployed to any location (simulated by the VM)
* Installation is automated and consistent (via Kickstart)
* The appliance operates in a disconnected environment (isolated network)
* This same ISO can be deployed via USB, network share, or PXE boot to ManufacturingCo's factory locations

[#verify_disconnected]
== 3.3 Show: Verify Disconnected Operation

=== Why We're Doing This

This proves the appliance works without network access. By verifying that container images were "already present on machine" rather than pulled from registries, we confirm that ManufacturingCo's appliances can operate completely offline in their factory locations.

=== Find the VM IP Address and Connect via SSH

Before you can verify the appliance, you need to find its IP address and connect to it.

. From the bastion host, check that the VM is running:
+
[source,sh,role=execute]
----
sudo virsh list --all
----

. Find the IP address of the VM:
+
[source,sh,role=execute]
----
sudo virsh domifaddr microshift-4.19-bootc-vm1
----
+
The output will show the IP address assigned to the VM. Note the IP address (for example, `192.168.100.165`).

. Connect to the VM via SSH using the IP address:
+
[source,sh,role=execute]
----
ssh redhat@<VM_IP_ADDRESS>
----
+
When prompted, enter the password: `redhat02`
+
[NOTE]
====
The default user is `redhat` with password `redhat02`. You should see a message indicating "Boot Status is GREEN - Health Check SUCCESS" when you successfully connect.
====

=== Check bootc System Status

. Confirm the system is running the 4.19 deployment:
+
[source,sh,role=execute]
----
sudo bootc status
----

=== Set Up kubeconfig for MicroShift Access

Before you can interact with MicroShift using `oc` commands, you need to configure the kubeconfig file. MicroShift generates a local kubeconfig file that you can use to access the cluster.

. Create the `.kube` directory in your home directory if it doesn't exist:
+
[source,sh,role=execute]
----
mkdir -p ~/.kube
----

. Copy the MicroShift kubeconfig file to your home directory:
+
[source,sh,role=execute]
----
sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig > ~/.kube/config
----

. Set the correct permissions on the kubeconfig file:
+
[source,sh,role=execute]
----
chmod go-r ~/.kube/config
----

[NOTE]
====
The kubeconfig file is automatically generated by MicroShift and contains the necessary credentials to access the cluster. For more information about kubeconfig files and remote access, see the link:https://docs.redhat.com/en/documentation/red_hat_build_of_microshift/4.19/html/configuring/microshift-node-access-kubeconfig[Red Hat build of MicroShift documentation]. For general information about deploying bootc images, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html-single/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/index#deploying-bootc-images_using-image-mode-for-rhel[Deploying bootc images].
====

=== Verify MicroShift and Applications

. Verify the node is ready:
+
[source,sh,role=execute]
----
oc get nodes
----

. Check that WordPress and MySQL pods are running:
+
[source,sh,role=execute]
----
oc get pods -A
----

=== Verify Embedded Container Images Are Being Used

This is the critical verification that proves the appliance is operating in disconnected mode.

. Ensure the images came from the ISO (not pulled from the network):
+
[source,sh,role=execute]
----
sudo crictl images
----

. Verify that Kubernetes events show images were already present (not pulled from network):
+
[source,sh,role=execute]
----
oc get events -A | grep -i pull
----
+
You should see events indicating that container images were "already present on machine" for:

* WordPress and MySQL application images
* MicroShift component images (from the openshift-release-dev registry)

[NOTE]
====
This confirms your appliance is fully **air-gapped** and relying on the embedded images you packaged inside the ISO. The Kubernetes events show "already present on machine" rather than "Successfully pulled", which indicates the images were loaded from the embedded storage during MicroShift startup, not pulled from external registries.
====

=== How This Solves the Problem

By verifying that images were "already present on machine", we've proven that:

* The appliance operates completely offline—no registry access required
* Embedded images are working correctly—all applications use embedded images
* The disconnected operations challenge is solved—ManufacturingCo can deploy to any location regardless of connectivity
* The appliance matches exactly what was validated—deterministic behavior

This verification confirms that ManufacturingCo's factory locations can operate their appliances without any network dependency.

[#validation]
== 3.4 Validation: Appliance Operating Successfully

=== Comprehensive Verification

Verify all components are functioning correctly:

. Confirm bootc status shows 4.19:
+
[source,sh,role=execute]
----
sudo bootc status
----

. Verify MicroShift node is ready:
+
[source,sh,role=execute]
----
oc get nodes
----

. Check that application pods are running:
+
[source,sh,role=execute]
----
oc get pods -A -o wide
----

. Verify embedded images are present (not pulled from network):
+
[source,sh,role=execute]
----
sudo crictl images | grep -E "wordpress|mysql"
----

. Verify Kubernetes events confirm images were already present:
+
[source,sh,role=execute]
----
oc get events -A | grep -i pull
----
+
Look for events showing "already present on machine" for application and MicroShift component images.

=== Expected Results

[IMPORTANT]
====
**Expected results:**

* `bootc status` should show the 4.19 image as active
* Node should be in "Ready" state
* WordPress and MySQL pods should be running in the `wordpress` namespace
* Container images (`wordpress:6.2.1-apache` and `mysql:8.0`) should be present in `crictl images` output
* Kubernetes events should show "already present on machine" for all images, confirming they were loaded from embedded storage, not pulled from external registries

If any pods are not running, check logs with `oc logs <pod-name> -n wordpress` or review MicroShift logs with `sudo journalctl -u microshift`.
====

=== Validation Checklist

□ VM is running and accessible via SSH  
□ `bootc status` shows 4.19 image as active  
□ MicroShift node is in "Ready" state  
□ WordPress and MySQL pods are running  
□ Container images are present in `crictl images`  
□ Kubernetes events show "already present on machine" (not "Successfully pulled")  
□ Appliance operates without network access

=== Exploring the WordPress Manifests (Optional)

To understand how the application is deployed, you can explore the manifests:

. View the Kustomization file:
+
[source,sh,role=execute]
----
cat wordpress/kustomization.yaml
----
+
[%collapsible]
====
[source,yaml]
----
resources:
  - wordpress.yaml
  - mysql.yaml
namespace: wordpress
----
====

. Inspect the WordPress deployment:
+
[source,sh,role=execute]
----
cat wordpress/wordpress.yaml
----
+
[%collapsible]
====
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: wordpress
          image: docker.io/library/wordpress:6.2.1-apache
          ports:
            - containerPort: 80
          env:
            - name: WORDPRESS_DB_HOST
              value: mysql
----
====

[NOTE]
====
MicroShift runs this container using the **embedded image**, not pulling from Docker Hub.
====

---

**Next:** Proceed to xref:module-04.adoc[Module 4: Safe Updates & Rollbacks] to learn how to update appliances safely with atomic updates and instant rollback capability.
